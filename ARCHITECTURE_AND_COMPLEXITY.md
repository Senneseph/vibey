# Architecture & Reasoning Complexity Analysis

## Current Architecture

### Layer 1: User Interface (Webview)
```
ChatPanel.ts (VS Code Extension)
    â†“
Webview HTML + CSS + JavaScript
    â”œâ”€ main.js (entry point)
    â”œâ”€ chat_manager.js (message handling)
    â”œâ”€ message_renderer.js (display updates)
    â”œâ”€ events.js (user interaction)
    â””â”€ task_manager.js (task tracking)
```

### Layer 2: Agent Orchestration
```
AgentOrchestrator (main reasoning loop)
    â”œâ”€ System Prompt (600 tokens, clean)
    â”œâ”€ Context Manager (file reading, caching)
    â”œâ”€ LLM Interface (Ollama)
    â”œâ”€ Tool Gateway (read_file, write_file, etc.)
    â””â”€ Reasoning Loop (256 max turns)
```

### Layer 3: LLM Integration
```
Ollama (Local LLM)
    â”œâ”€ Model: Qwen3-coder-32b (or configured)
    â”œâ”€ Streaming: Disabled (full response at once)
    â””â”€ Token reporting: prompt_eval_count + eval_count
```

## Request Flow

```
1. USER INPUT
   â†“
2. WEBVIEW SENDS MESSAGE
   - Message text
   - Context files (optional)
   â†“
3. EXTENSION PROCESSES
   - Loads history from disk
   - Resolves context file contents
   - Estimates tokens (send count)
   â†“
4. REPORTS CONTEXT TO UI
   ğŸ“ Context Added (~X tokens)
   â†“
5. CALLS LLM (Orchestrator.chat)
   - Adds context to history
   - Enters reasoning loop
   - Max 256 turns
   â†“
6. REASONING LOOP
   - Send to LLM
   - Parse response (JSON or text)
   - If tool calls: execute tools
   - If final response: break loop
   â†“
7. REPORTS PROGRESS TO UI
   - Thinking updates ("Analyzing...")
   - Thought updates (reasoning)
   - Tool execution (start/end)
   â†“
8. RECEIVES LLM RESPONSE
   - Gets final text
   - Estimates tokens (receive count)
   â†“
9. REPORTS COMPLETION
   ğŸ“Š Token Usage: X sent, Y received
   â†“
10. SAVES HISTORY & DISPLAYS
    - Message added to history
    - Saved to disk
    - Displayed in chat
```

## Reasoning Complexity - Before & After

### BEFORE (v0.4.5)
System prompt included:
- âœ— Context Caching instructions (200 tokens)
- âœ— Iterative Problem-Solving (300 tokens)
- âœ— Checkpointing (150 tokens)
- âœ— Master Context Management (400 tokens)
- âœ— Task Progress Tracking (250 tokens)
- âœ— Example workflows (300 tokens)
- âœ— Context Format explanations (200 tokens)

**Total system prompt: ~2,200 tokens**

Impact:
- Every request had to consume 2,200 tokens just for system prompt
- Agent was "told" about capabilities it didn't actually use
- LLM had to parse and understand unused instructions
- Extra cognitive overhead for the model

### AFTER (v0.4.7)
System prompt now includes ONLY:
- âœ“ Core behavior (autonomous, DO IT approach)
- âœ“ Tool definitions (what it CAN do)
- âœ“ Response format (how to respond)
- âœ“ Key rules (reading, minimal changes, etc.)

**Total system prompt: ~600 tokens**

Benefit:
- **1,600 tokens saved per request** (73% reduction)
- Clearer instructions = more consistent responses
- Faster processing = quicker responses
- Same capabilities, less overhead

## Why Requests Timeout

### Scenario: "GPU is being used but request times out"

Possible causes (in order of likelihood):

1. **Context Too Large** (Most Common)
   - Problem: You included many large files
   - Symptom: "Context Added (~16K tokens)" in UI
   - Solution: Reduce context, be more specific
   - Fix time: Model won't even start until it parses all input

2. **Complex Tool Sequences** (Second Most Common)
   - Problem: Agent is executing 10+ tools in sequence
   - Symptom: Multiple tool execution updates appearing
   - Solution: Ask for simpler tasks, break into steps
   - Fix time: Each tool adds 2-10 seconds

3. **Model Thinking Hard** (Legitimate)
   - Problem: Genuinely complex reasoning required
   - Symptom: "Turn N: Reasoning..." updates appear
   - Solution: Simplify request or upgrade GPU/model
   - Fix time: Model is actually working, let it finish

4. **Network Latency**
   - Problem: Ollama connection is slow
   - Symptom: UI shows updates but very slowly
   - Solution: Check network, restart Ollama
   - Fix time: Not really a fix, just slow

5. **True Timeout** (Rare, indicates bug)
   - Problem: Request genuinely stalled
   - Symptom: No updates for >30 seconds despite "Analyzing..."
   - Solution: Check browser console, restart extension
   - Fix time: This is a bug we need to find

## Debugging With New Feedback

### Scenario: Request hangs after 30 seconds

```
You: Analyze these files [adds 10 files]

ğŸ“ Context Added (~10,240 tokens) â† Check: Is this reasonable?
   If >15K, context is too large

Vibey: [Status shows "Analyzing..."]

[Wait 30 seconds - nothing updates]

ğŸ¤” Analyzing request...        â† Stuck here?
   â†“
   Open Webview Dev Tools
   â†’ Console tab
   â†’ Check for JS errors
   â†’ Check browser network tab
```

### Scenario: Request takes 2 minutes

```
You: Analyze these files

ğŸ“ Context Added (~4,096 tokens) â† Reasonable

ğŸ¤” Analyzing request...

[Tools start executing]
  âœ… read_file (3s)
  âœ… read_file (2s)
  ğŸ› ï¸ run_command (â³ running...)
  [5 seconds, 10 seconds, 20 seconds...]

â†’ This tool is slow (probably network/GPU)
â†’ Legitimate work being done
â†’ GPU IS being used (you can verify in system monitor)
â†’ Just slow at this moment
```

## Optimization Tips

### If Context Size is Problem
```
Context showing: ğŸ“ Context Added (~15,000 tokens)

â†“ DO THIS:

Instead of:
  read_file(src/app.ts)
  read_file(src/types.ts)
  read_file(src/utils.ts)
  read_file(src/helpers.ts)

Try:
  read_file(src/app.ts) only - the main file
  
Let Vibey ask for other files if needed
```

### If Tool Execution is Slow
```
If you see: ğŸ› ï¸ run_command (â³ 30 seconds)

This is ACTUAL GPU WORK
Not a bug
Just slow

You can:
1. Wait for it to complete âœ“
2. Kill the request (button in UI)
3. Upgrade your GPU/Model
4. Ask simpler questions
```

### If Thinking is Complex
```
If you see: Turn 5: Reasoning...
            Turn 6: Reasoning...
            Turn 7: Reasoning...

This is the agent re-thinking (planning, adjusting)
Normal behavior
Not a problem

Means:
- Agent is being thorough âœ“
- Probably found something tricky âœ“
- Working to get it right âœ“
```

## Token Usage Examples

### Simple Request
```
You: "Create a function that adds two numbers"

ğŸ“ Context Added: None
ğŸ“Š Token Usage: 512 sent, 256 received

Analysis:
- Small input, no context
- Fast response expected
- Very efficient
```

### Medium Request with Context
```
You: "Fix the bug in this file" 
[Adds 1 file, 5KB]

ğŸ“ Context Added (~1,280 tokens)
ğŸ“Š Token Usage: 2,048 sent, 512 received

Analysis:
- Input + context reasonable
- Response is moderate
- Normal performance
```

### Large Request with Heavy Context
```
You: "Refactor this codebase" 
[Adds 5 files, 50KB each]

ğŸ“ Context Added (~12,800 tokens)
ğŸ“Š Token Usage: 14,000 sent, 2,048 received

Analysis:
- Large context
- Model had lots to read first
- Longer processing expected
- Response might be incomplete (too many tokens)
  â†’ Might need to split into multiple requests
```

## Future Optimization Ideas

### Short-term (Easy)
- [ ] Exact token counting (using tokenizer library)
- [ ] Per-file token display
- [ ] Tool execution timing
- [ ] Estimated time remaining

### Medium-term (Moderate)
- [ ] Smart context pruning (remove unused files)
- [ ] Request queueing with priority
- [ ] Token limit warnings
- [ ] Automatic context splitting

### Long-term (Complex)
- [ ] Streaming responses (show results as they come)
- [ ] Parallel tool execution
- [ ] Model selection based on task complexity
- [ ] Cost estimation per request

